








This project is a web scraping application built with Python and Flask. The application scrapes text and links from a given website and saves the scraped data to text files. The text data is also preprocessed using various text preprocessing techniques.

Here's a brief overview of the code structure:

1. `main.py`: This is the entry point of the application. It starts the Flask server and runs the application.

2. `web_ui.py`: This file contains the Flask routes and the main logic of the application. It handles the form submission from the user, performs the web scraping and text preprocessing, and saves the scraped data to files.

3. `common/file_utils.py`: This file contains utility functions for file operations, such as creating a directory, finding a unique file name, and saving text and links to files.

4. `common/web_utils.py`: This file contains utility functions for web operations, such as sending a GET request, parsing HTML, extracting text and links from a website, and getting the website name from a URL.

5. `common/text_preprocessing.py`: This file contains functions for text preprocessing, such as lowercasing text, removing punctuation and whitespaces, tokenizing text, removing stopwords, and stemming tokens.

The application follows a clear and organized folder structure. The `src` folder contains the source code of the application, the `templates` folder contains the HTML templates for the Flask application, and the `data` folder contains the scraped data. The `data` folder is further divided into `raw_data`, `preprocessed_data`, and `links` subfolders, which contain the raw scraped text, the preprocessed text, and the scraped links, respectively.

The application is designed to be easy to use and flexible. The user can choose to scrape either text, links, or both from a website, and the scraped data is saved to separate files for each website. The text data is preprocessed before being saved, which makes it ready for further analysis or machine learning tasks.

The project is hosted on GitHub and includes a README file with instructions on how to run the application. The code is well-documented and follows good coding practices, making it easy to understand and maintain.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Project is a Flask web application that allows users to scrape text and web links from a given website. 
The scraped data is saved in separate directories within the `data` folder. 
The code is organized into separate modules (`web_ui.py`, `web_utils.py`, and `file_utils.py`) to handle different aspects of the scraping process.

1. `home.html`: This is the HTML template for the web scraping application's home page. It contains a form where users can enter a website URL and select checkboxes to indicate whether they want to scrape text or web links from the website. There is also a "Scrape" button to initiate the scraping process. If the scraping process is completed, an "End" button is displayed.

2. `main.py`: This is the main entry point of your Flask application. It imports the `app` object from `web_ui.py` and runs the Flask server when the script is executed.

3. `web_ui.py`: This file contains the Flask routes and logic for handling the web scraping functionality. The `home()` function is the route for the home page, where the form submission is processed. It retrieves the website URL and the values of the checkboxes from the form. Depending on the checkbox values, it performs the scraping and saving logic using functions from `common.file_utils` and `common.web_utils`. If the scraping process is completed, the `scraping_finished` flag is set to `True` to display the "End" button. The `end()` function is a route that redirects to a blank page to simulate closing the web page. The Flask server is also opened automatically in the default web browser.

4. `web_utils.py`: This file contains utility functions for sending HTTP GET requests, parsing HTML using BeautifulSoup, and extracting text and links from the website.

5. `file_utils.py`: This file contains utility functions for creating directories, finding unique file names, and saving text and links to files.

6. 'text_filtering.py': This file contains functions for text filtering, such as removing special characters, punctuation and converting text to lowercase.

7. `data` folder: This folder contains three subfolders, `links`, `preprocessed_data`, and `raw_data`, where the scraped web links, preprocessed text data, and raw text data are saved, respectively. The file names are generated based on the website name and a unique number to avoid overwriting existing files.

7. `templates` folder: This folder contains the HTML templates used by the Flask application. Currently, it only contains the `home.html` template.

8. `admin` folder: This folder contains a script (`create_folder_structure.py`) that can be used to create the initial folder structure for the project.

9. `docs` folder: This folder contains a text file (`folder_structure.txt`) that documents the folder structure of the project.

